{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "realistic-journal",
   "metadata": {},
   "source": [
    "## Step 1: Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "charming-addition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import pyspark\n",
    "\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc, size, max, abs, split, col\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "actual-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Predictive Maintenance\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "spark = init_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "systematic-terrain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---+\n",
      "|machineID| model|age|\n",
      "+---------+------+---+\n",
      "|        1|model3| 18|\n",
      "|        2|model4|  7|\n",
      "|        3|model3|  8|\n",
      "|        4|model3|  7|\n",
      "|        5|model3|  2|\n",
      "|        6|model3|  7|\n",
      "|        7|model3| 20|\n",
      "|        8|model3| 16|\n",
      "|        9|model4|  7|\n",
      "|       10|model3| 10|\n",
      "|       11|model2|  6|\n",
      "|       12|model3|  9|\n",
      "|       13|model1| 15|\n",
      "|       14|model3|  1|\n",
      "|       15|model3| 14|\n",
      "|       16|model1|  3|\n",
      "|       17|model1| 14|\n",
      "|       18|model3| 15|\n",
      "|       19|model3| 17|\n",
      "|       20|model2| 16|\n",
      "+---------+------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = \"..\\data\\machines.csv\"\n",
    "machines = spark.read.csv(filename, sep=',', header=True)\n",
    "\n",
    "machines.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bigger-shell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3919\n",
      "+--------------------+---------+-------+\n",
      "|            datetime|machineID|errorID|\n",
      "+--------------------+---------+-------+\n",
      "| 1/3/2015 7:00:00 AM|        1| error1|\n",
      "| 1/3/2015 8:00:00 PM|        1| error3|\n",
      "| 1/4/2015 6:00:00 AM|        1| error5|\n",
      "|1/10/2015 3:00:00 PM|        1| error4|\n",
      "|1/22/2015 10:00:0...|        1| error4|\n",
      "|1/25/2015 3:00:00 PM|        1| error4|\n",
      "|1/27/2015 4:00:00 AM|        1| error1|\n",
      "|3/3/2015 10:00:00 PM|        1| error2|\n",
      "| 3/5/2015 6:00:00 AM|        1| error1|\n",
      "|3/20/2015 6:00:00 PM|        1| error1|\n",
      "|3/26/2015 1:00:00 AM|        1| error2|\n",
      "|3/31/2015 11:00:0...|        1| error1|\n",
      "|4/19/2015 6:00:00 AM|        1| error2|\n",
      "|4/19/2015 6:00:00 AM|        1| error3|\n",
      "|4/29/2015 7:00:00 PM|        1| error4|\n",
      "|5/4/2015 11:00:00 PM|        1| error2|\n",
      "|5/12/2015 9:00:00 AM|        1| error1|\n",
      "|5/21/2015 7:00:00 AM|        1| error4|\n",
      "|5/24/2015 2:00:00 AM|        1| error3|\n",
      "|5/25/2015 5:00:00 AM|        1| error1|\n",
      "+--------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = \"..\\data\\errors.csv\"\n",
    "errors = spark.read.csv(filename, sep=',', header=True)\n",
    "\n",
    "print(errors.count())\n",
    "errors.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "loving-plenty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "876100\n",
      "+--------------------+---------+----------------+----------------+----------------+----------------+\n",
      "|            datetime|machineID|            volt|          rotate|        pressure|       vibration|\n",
      "+--------------------+---------+----------------+----------------+----------------+----------------+\n",
      "| 1/1/2015 6:00:00 AM|        1|176.217853015625|418.504078221616|113.077935462083|45.0876857639276|\n",
      "| 1/1/2015 7:00:00 AM|        1| 162.87922289706|402.747489565395|95.4605253823187|43.4139726834815|\n",
      "| 1/1/2015 8:00:00 AM|        1|170.989902405567|527.349825452291|75.2379048586662|34.1788471214451|\n",
      "| 1/1/2015 9:00:00 AM|        1|162.462833264092|346.149335043074|109.248561276504|41.1221440884256|\n",
      "|1/1/2015 10:00:00 AM|        1| 157.61002119306|435.376873016938|111.886648210168|25.9905109982024|\n",
      "|1/1/2015 11:00:00 AM|        1|172.504839196295|430.323362106675|95.9270416939636|35.6550173268837|\n",
      "|1/1/2015 12:00:00 PM|        1|156.556030606329|499.071623068962|111.755684290096|42.7539196974773|\n",
      "| 1/1/2015 1:00:00 PM|        1|172.522780814836|409.624717000438| 101.00108276407|35.4820086610704|\n",
      "| 1/1/2015 2:00:00 PM|        1|175.324523915223|398.648780707752|110.624360548654|45.4822868466294|\n",
      "| 1/1/2015 3:00:00 PM|        1|169.218423246933|460.850669930244|104.848229967003|39.9017354356787|\n",
      "| 1/1/2015 4:00:00 PM|        1|167.060980719256|382.483542906686|103.780662505568|42.6757999060571|\n",
      "| 1/1/2015 5:00:00 PM|        1|160.263953725914|448.084255968416|96.4809756730127|38.5436809273919|\n",
      "| 1/1/2015 6:00:00 PM|        1|153.353491529019|490.672921027229|86.0124402476987|44.1085543746712|\n",
      "| 1/1/2015 7:00:00 PM|        1|182.739113039826|418.199251966527|93.4849540613739|41.3671897268129|\n",
      "| 1/1/2015 8:00:00 PM|        1| 170.33543789114|402.461186552994|93.2357870560117|39.7398826787247|\n",
      "| 1/1/2015 9:00:00 PM|        1|182.467109259603|501.918972726944|85.7626146951866|51.0214861151087|\n",
      "|1/1/2015 10:00:00 PM|        1|151.335682229837|444.922656455828|94.2473713287117|42.1196520943364|\n",
      "|1/1/2015 11:00:00 PM|        1|172.535396206493|511.886364263088|91.3294290707084|32.2488323711174|\n",
      "|1/2/2015 12:00:00 AM|        1|180.097494604215|486.712303934437|96.7339151541194|38.8967757315964|\n",
      "| 1/2/2015 1:00:00 AM|        1|169.605854353272|519.452812074631|78.8807795743296|40.1568749074533|\n",
      "+--------------------+---------+----------------+----------------+----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = \"..\\data\\\\telemetry.csv\"\n",
    "telemetry = spark.read.csv(filename, sep=',', header=True)\n",
    "\n",
    "print(telemetry.count())\n",
    "telemetry.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-thursday",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "bigdata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
