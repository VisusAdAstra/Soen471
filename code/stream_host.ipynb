{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "skilled-accordance",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+\n",
      "| id|label|               tweet|\n",
      "+---+-----+--------------------+\n",
      "|  1|    0| @user when a fat...|\n",
      "|  2|    0|@user @user thank...|\n",
      "|  3|    0|  bihday your maj...|\n",
      "|  4|    0|#model   i love u...|\n",
      "|  5|    0| factsguide: soci...|\n",
      "+---+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing required libraries\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "import pyspark.sql.types as tp\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# initializing spark session\n",
    "sc = SparkContext(appName=\"PySparkShell\")\n",
    "spark = SparkSession(sc)\n",
    "    \n",
    "# define the schema\n",
    "my_schema = tp.StructType([\n",
    "  tp.StructField(name= 'id',          dataType= tp.IntegerType(),  nullable= True),\n",
    "  tp.StructField(name= 'label',       dataType= tp.IntegerType(),  nullable= True),\n",
    "  tp.StructField(name= 'tweet',       dataType= tp.StringType(),   nullable= True)\n",
    "])\n",
    "    \n",
    "# read the dataset  \n",
    "my_data = spark.read.csv('twitter_sentiments.csv',\n",
    "                         schema=my_schema,\n",
    "                         header=True)\n",
    "\n",
    "# view the data\n",
    "my_data.show(5)\n",
    "\n",
    "# print the schema of the file\n",
    "my_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "seeing-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stage 1: tokenize the tweet text    \n",
    "stage_1 = RegexTokenizer(inputCol= 'tweet' , outputCol= 'tokens', pattern= '\\\\W')\n",
    "# define stage 2: remove the stop words\n",
    "stage_2 = StopWordsRemover(inputCol= 'tokens', outputCol= 'filtered_words')\n",
    "# define stage 3: create a word vector of the size 100\n",
    "stage_3 = Word2Vec(inputCol= 'filtered_words', outputCol= 'vector', vectorSize= 100)\n",
    "# define stage 4: Logistic Regression Model\n",
    "model = LogisticRegression(featuresCol= 'vector', labelCol= 'label')\n",
    "\n",
    "# define a function to compute sentiments of the received tweets\n",
    "def get_prediction(tweet_text):\n",
    "\ttry:\n",
    "    # filter the tweets whose length is greater than 0\n",
    "\t\ttweet_text = tweet_text.filter(lambda x: len(x) > 0)\n",
    "    # create a dataframe with column name 'tweet' and each row will contain the tweet\n",
    "\t\trowRdd = tweet_text.map(lambda w: Row(tweet=w))\n",
    "    # create a spark dataframe\n",
    "\t\twordsDataFrame = spark.createDataFrame(rowRdd)\n",
    "    # transform the data using the pipeline and get the predicted sentiment\n",
    "\t\tpipelineFit.transform(wordsDataFrame).select('tweet','prediction').show()\n",
    "\texcept : \n",
    "\t\tprint('No data')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lyric-russia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the pipeline\n",
    "pipeline = Pipeline(stages= [stage_1, stage_2, stage_3, model])\n",
    "\n",
    "# fit the pipeline model with the training data\n",
    "pipelineFit = pipeline.fit(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-trainer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "+-----+----------+\n",
      "|tweet|prediction|\n",
      "+-----+----------+\n",
      "|hello|       0.0|\n",
      "+-----+----------+\n",
      "\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "+-----+----------+\n",
      "|tweet|prediction|\n",
      "+-----+----------+\n",
      "|    a|       0.0|\n",
      "+-----+----------+\n",
      "\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n"
     ]
    }
   ],
   "source": [
    "# initialize the streaming context \n",
    "ssc = StreamingContext(sc, batchDuration= 3)\n",
    "\n",
    "# Create a DStream that will connect to hostname:port, like localhost:9991\n",
    "lines = ssc.socketTextStream(\"localhost\", 9991)\n",
    "\n",
    "# split the tweet text by a keyword 'TWEET_APP' so that we can identify which set of words is from a single tweet\n",
    "words = lines.flatMap(lambda line : line.split('TWEET_APP'))\n",
    "\n",
    "# get the predicted sentiments for the tweets received\n",
    "words.foreachRDD(get_prediction)\n",
    "\n",
    "# Start the computation\n",
    "ssc.start()             \n",
    "\n",
    "# Wait for the computation to terminate\n",
    "ssc.awaitTermination()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-wednesday",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "bigdata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
